#!/usr/bin/env python
"""
Benchmark SOTA Scaling & Quality Evaluation (Table 1 & Table 2)
===============================================================
1. ì§€ì •ëœ í”„ë¡¬í”„íŠ¸ë¡œ AR, FreeNoise, QSFM ëª¨ë¸ì˜ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. ê° K(ìƒ· ìˆ˜)ë³„ë¡œ VRAMê³¼ ìƒì„± ì†ë„(Denoising Time)ë¥¼ ì¸¡ì •í•˜ì—¬ Table 2ë¥¼ ë§Œë“­ë‹ˆë‹¤.
3. ìƒì„±ì´ ëë‚œ ì§í›„, ê° ì¶œë ¥ í´ë”ì— ëŒ€í•´ `eval_multi_shot_metrics.py`ë¥¼ í˜¸ì¶œí•˜ì—¬
   ë¹„ë””ì˜¤ í’ˆì§ˆ ë° ì»·í¸ì§‘ ì¼ê´€ì„± ì§€í‘œ(CLIPSIM, DINO)ë¥¼ ìë™ ì¸¡ì •í•©ë‹ˆë‹¤ (Table 1).
"""

import subprocess
import re
import csv
import sys
import json
from pathlib import Path

# ==========================================
# âš™ï¸ 1. ì‹¤í—˜ ì„¸íŒ… ë° í”„ë¡¬í”„íŠ¸ ì •ì˜
# ==========================================
K_VALUES = [4, 8, 16] # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ 32ëŠ” í•„ìš”ì‹œ ì¶”ê°€í•˜ì„¸ìš”.

METHODS = {
    "Auto-regressive": "scripts/run_autoregressive_inference.py",
    "FreeNoise": "scripts/run_free_noise_inference.py",
    "QSFM": "scripts/run_qsfm_inference.py",
}

# ğŸŒŸ ìˆœì • ë² ì´ìŠ¤ ëª¨ë¸ (ë¼ˆëŒ€)
BASE_MODEL = "LTXV_2B_0.9.6_DEV"

BASELINE_LORA_DIR = Path("/home/dongwoo43/qfm/LTX-Video-Trainer/outputs/ltxv_lora/checkpoints")
QSFM_LORA_DIR = Path("/home/dongwoo43/qfm/LTX-Video-Trainer/outputs/qsfm_lora/checkpoints")
OUTPUT_CSV = "outputs/benchmark_results.csv"

# ğŸ° ì‚¬ìš©ìê°€ ì§€ì •í•œ 4ê°œì˜ í‘œì¤€ í”„ë¡¬í”„íŠ¸
STANDARD_PROMPTS = [
    "A cartoon rabbit waddles through an open meadow as small animated birds circle overhead.",
    "A large fluffy rabbit sits near a pond surrounded by trees in an animated nature scene.",
    "Three squirrels fly through the air over a forest as a giant rabbit watches with curiosity.",
    "An animated rabbit chases a small rodent through tall grass in a colorful cartoon forest.",
]

def get_latest_lora(checkpoint_dir: Path):
    """í´ë” ë‚´ì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ ìŠ¤í…ì˜ .safetensors íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
    if not checkpoint_dir.exists():
        return None
    ckpts = list(checkpoint_dir.glob("*.safetensors"))
    if not ckpts:
        return None
    ckpts.sort(key=lambda x: int(x.stem.split("_")[-1]) if "_" in x.stem else 0)
    return ckpts[-1]

def parse_output(output: str):
    """í‘œì¤€ ì¶œë ¥ì—ì„œ VRAMê³¼ ìƒì„± ì†ë„ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    metrics = {
        "Peak VRAM (GB)": 0.0,
        "Denoising Time (s)": 0.0,
        "Total Time (s)": 0.0,
        "OOM": False
    }

    if "CUDA out of memory" in output or "OOM Predicted" in output or "out of memory" in output.lower():
        metrics["OOM"] = True
        return metrics

    vram_matches = re.findall(r"Peak VRAM: ([\d\.]+) GB", output)
    if vram_matches:
        metrics["Peak VRAM (GB)"] = max(float(v) for v in vram_matches)

    dt_match = re.search(r"\[METRICS\] Denoising Time per Shot: ([\d\.]+)", output)
    if dt_match:
        metrics["Denoising Time (s)"] = float(dt_match.group(1))

    tt_match = re.search(r"\[METRICS\] Total Generation Time: ([\d\.]+)", output)
    if tt_match:
        metrics["Total Time (s)"] = float(tt_match.group(1))

    return metrics

def run_benchmark():
    results = [] 

    baseline_lora_path = get_latest_lora(BASELINE_LORA_DIR)
    qsfm_lora_path = get_latest_lora(QSFM_LORA_DIR)

    print("=" * 80)
    print("ğŸš€ SOTA Macro-Benchmark & Quality Evaluation Pipeline")
    print(f" - Base Model: {BASE_MODEL}")
    print(f" - Baseline LoRA: {baseline_lora_path}")
    print(f" - QSFM LoRA: {qsfm_lora_path}")
    print("=" * 80)
    print(f"{'Method':<20} | {'K':<4} | {'VRAM (GB)':<10} | {'Time/Shot (s)':<15} | {'Status'}")
    print("-" * 70)

    for method_name, script_path in METHODS.items():
        for K in K_VALUES:
            output_dir = Path(f"eval_workspace/benchmark/{method_name.lower().replace(' ', '_')}_k{K}")
            output_dir.mkdir(parents=True, exist_ok=True)

            cmd = [sys.executable, script_path]
            cmd.extend(["--output_dir", str(output_dir)])
            cmd.extend(["--seed", "42"])
            cmd.extend(["--steps", "30"]) # ë…¼ë¬¸ìš© í’ˆì§ˆì„ ìœ„í•´ 30ìŠ¤í… ê¶Œì¥

            cmd.extend(["--model_source", BASE_MODEL])

            if method_name in ["Auto-regressive", "FreeNoise"] and baseline_lora_path:
                cmd.extend(["--lora_weights_path", str(baseline_lora_path)])
            elif method_name == "QSFM" and qsfm_lora_path:
                cmd.extend(["--lora_weights_path", str(qsfm_lora_path)])

            # ==========================================
            # ğŸ“ 2. ê° ë©”ì†Œë“œì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ë¶„ë°°
            # ==========================================
            prompts_file = output_dir / "prompts.json"
            
            if method_name == "Auto-regressive":
                # ARì€ Kê°œì— ë§ì¶° í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜ë³µ/í• ë‹¹
                cycled_prompts = [STANDARD_PROMPTS[i % len(STANDARD_PROMPTS)] for i in range(K)]
                prompts_data = {"standard": [{"prompt": p} for p in cycled_prompts]}
            else:
                # FreeNoiseì™€ QSFMì€ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ì—ì„œ Kìƒ·ì„ ë½‘ê±°ë‚˜, ë‚´ë¶€ì ìœ¼ë¡œ Kë¥¼ ì¡°ì ˆí•¨
                prompts_data = {"standard": [{"prompt": STANDARD_PROMPTS[0]}]}
                cmd.extend(["--num_shots", str(K)])
                
            prompts_file.write_text(json.dumps(prompts_data))
            cmd.extend(["--prompts_json", str(prompts_file)])

            # ==========================================
            # ğŸƒ 3. ë¹„ë””ì˜¤ ìƒì„± ì‹¤í–‰ (í•˜ë“œì›¨ì–´ ì§€í‘œ ì¸¡ì •)
            # ==========================================
            status = "Unknown"
            metrics = parse_output("")

            try:
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=2000)
                metrics = parse_output(result.stdout)

                if result.returncode != 0 and not metrics["OOM"]:
                    status = "Error"
                    print(f"Error in {method_name} K={K}:\n{result.stderr[-300:]}")
                elif metrics["OOM"]:
                    status = "OOM"
                    metrics["Peak VRAM (GB)"] = "OOM"
                    metrics["Denoising Time (s)"] = "-"
                else:
                    status = "Success"

            except subprocess.TimeoutExpired:
                status = "Timeout"
            except Exception as e:
                status = f"Error: {e}"

            vram_display = f"{metrics['Peak VRAM (GB)']:.2f}" if isinstance(metrics['Peak VRAM (GB)'], float) else str(metrics['Peak VRAM (GB)'])
            time_display = f"{metrics['Denoising Time (s)']:.2f}" if isinstance(metrics['Denoising Time (s)'], float) else str(metrics['Denoising Time (s)'])
            print(f"{method_name:<20} | {K:<4} | {vram_display:<10} | {time_display:<15} | {status}")

            results.append({
                "Method": method_name, "K": K, "VRAM": metrics["Peak VRAM (GB)"],
                "Time_per_Shot": metrics["Denoising Time (s)"], "Total_Time": metrics["Total Time (s)"]
            })

            # ==========================================
            # ğŸ“Š 4. í’ˆì§ˆ ë° ì„±ëŠ¥ ì§€í‘œ ì¦‰ì‹œ ìë™ í‰ê°€ (Table 1 ìš©)
            # ==========================================
            if status == "Success":
                print(f"  â””â”€> [Evaluating] {method_name} K={K} ë¹„ë””ì˜¤ í’ˆì§ˆ ë¶„ì„ ì¤‘...")
                # ğŸš€ ìˆ˜ì • ì™„ë£Œ: --shots_dir ì™€ --prompts_json ì ìš©
                eval_cmd = [
                    sys.executable, "scripts/eval_multi_shot_metrics.py", 
                    "--shots_dir", str(output_dir),
                    "--prompts_json", str(prompts_file)
                ]
                try:
                    subprocess.run(eval_cmd, check=False)
                except Exception as eval_e:
                    print(f"      í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {eval_e}")

    # ==========================================
    # ğŸ’¾ 5. í•˜ë“œì›¨ì–´ ì§€í‘œ ê²°ê³¼ ì €ì¥ (Table 2 ìš©)
    # ==========================================
    Path(OUTPUT_CSV).parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_CSV, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["Method", "K", "VRAM", "Time_per_Shot", "Total_Time"])
        writer.writeheader()
        writer.writerows(results)

    print("\n" + "="*80)
    print("ğŸ“Š Table 2. Scaling Law (Hardware Efficiency)")
    print("="*80)
    header = f"{'Method':<20} | {'K=4 VRAM/Time':<20} | {'K=8 VRAM/Time':<20} | {'K=16 VRAM/Time':<20}"
    print(header)
    print("-" * len(header))

    method_data = {m: {} for m in METHODS.keys()}
    for row in results:
        m, k = row["Method"], row["K"]
        vram = f"{row['VRAM']:.1f}G" if isinstance(row['VRAM'], float) else row['VRAM']
        time = f"{row['Time_per_Shot']:.1f}s" if isinstance(row['Time_per_Shot'], float) else str(row['Time_per_Shot'])
        method_data[m][k] = f"{vram} / {time}"

    for m in METHODS.keys():
        row_str = f"{m:<20} | "
        for k in K_VALUES:
            val = method_data[m].get(k, "N/A")
            row_str += f"{val:<20} | "
        print(row_str)

    print(f"\nâœ… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! í•˜ë“œì›¨ì–´ ê²°ê³¼ëŠ” {OUTPUT_CSV} ì— ì €ì¥ë˜ì—ˆìœ¼ë©°, í’ˆì§ˆ í‰ê°€ ê²°ê³¼ëŠ” ìœ„ ì½˜ì†” ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    run_benchmark()